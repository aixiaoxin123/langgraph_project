å‚è€ƒæ–‡æ¡£æ•™ç¨‹ï¼š

https://langchain-ai.github.io/langgraph/tutorials/get-started/3-add-memory/



```python
# https://python.langchain.com/docs/integrations/tools/tavily_search/

# è·å–å¯†é’¥çš„åœ°å€ï¼š https://app.tavily.com/sign-in




import os
TAVILY_API_KEY="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"
os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY

```


```python
from langchain_tavily import TavilySearch

tool = TavilySearch(max_results=5)
tools = [tool]
tool.invoke("What's a 'node' in LangGraph?")

```




    {'query': "What's a 'node' in LangGraph?",
     'follow_up_questions': None,
     'answer': None,
     'images': [],
     'results': [{'title': "Mastering LangGraph: A Beginner's Guide to Building ... - Medium",
       'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',
       'content': 'Stateful Graph: LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph. We define nodes for classifying the input, handling greetings, and handling search queries. def classify_input_node(state): LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph.',
       'score': 0.69249696,
       'raw_content': None},
      {'title': 'LangGraph Basics: Understanding State, Schema, Nodes, and Edges',
       'url': 'https://medium.com/@vivekvjnk/langgraph-basics-understanding-state-schema-nodes-and-edges-77f2fd17cae5',
       'content': 'LangGraph Basics: Understanding State, Schema, Nodes, and Edges | by Story_Teller | Medium LangGraph Basics: Understanding State, Schema, Nodes, and Edges LangGraph Basics: Understanding State, Schema, Nodes, and Edges These predefined structures in the messaging app are synonymous with the schema of the state in LangGraph. Just as a messaging app ensures all interactions (messages) follow a consistent format, the schema in LangGraph ensures the state passed along edges is structured and interpretable. This static schema allows nodes to rely on a consistent state format, ensuring seamless communication along edges throughout the graph. In this article, we explored the foundational concepts of graph-based systems, drawing parallels to familiar messaging applications to illustrate how edges, nodes, and state transitions function seamlessly in dynamic workflows.',
       'score': 0.6501347,
       'raw_content': None},
      {'title': "An Absolute Beginner's Guide to LangGraph.js",
       'url': 'https://techcommunity.microsoft.com/blog/educatordeveloperblog/an-absolute-beginners-guide-to-langgraph-js/4212496',
       'content': "We'll add the name and isHuman properties to our State object and update the sayHello and sayBye nodes to use these State object properties. Update the Graph's nodes and edges: We'll build a graph that returns a random fact or joke based on the user's input. Finally, inside src/index.ts, import and execute the Graph with a user input, inside the /joke-or-fact route: In this guide, we've covered the basics of LangGraph.js, building a simple graph that returns a random fact or joke based on user input. We've learned how to define nodes, edges, and state objects and how to add conditional routing to our Graph. LangGraph.js is a powerful tool for building complex workflows and managing State in your applications.",
       'score': 0.59457535,
       'raw_content': None},
      {'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',
       'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',
       'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.',
       'score': 0.5010992,
       'raw_content': None},
      {'title': 'How to Build AI Agents with LangGraph: A Step-by-Step Guide',
       'url': 'https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832',
       'content': 'In this step, weâ€™ll define how the AI agent manages its state (the ongoing context of the conversation) and ensure it responds appropriately to the userâ€™s input and tool output. This involves creating a template for the conversation, specifying the tools that the assistant will use, and configuring how the AI agent will respond to user input and trigger different functions (like calculating solar savings). This step ensures that the AI assistant can access and trigger the tools as needed during the conversation, creating a seamless interaction between the user and the assistant. By following these steps, you have successfully created an AI assistant using LangGraph that can calculate solar panel energy savings based on user inputs.',
       'score': 0.0924337,
       'raw_content': None}],
     'response_time': 0.5}




```python
import os
from langchain.chat_models import init_chat_model

API_KEY = "sk-123"

BASE_URL = "https://api.deepseek.com"

os.environ["OPENAI_API_KEY"] = API_KEY
os.environ["OPENAI_API_BASE"] = BASE_URL


llm = init_chat_model("openai:deepseek-chat")
```


```python
#æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†å®ƒåˆå¹¶åˆ° ï¼šStateGraph

from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

# Modification: tell the LLM which tools it can call
# highlight-next-line
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
```




    <langgraph.graph.state.StateGraph at 0x21246095dc0>




```python
from typing import Annotated

from langchain_tavily import TavilySearch
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

tool = TavilySearch(max_results=2)
tools = [tool]
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
# Any time a tool is called, we return to the chatbot to decide the next step
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()
```


```python

```


```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
```


```python
graph = graph_builder.compile(checkpointer=memory)
```


```python
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
```


    
![png](3%E6%B7%BB%E5%8A%A0%E5%86%85%E5%AD%98_files/3%E6%B7%BB%E5%8A%A0%E5%86%85%E5%AD%98_9_0.png)
    



```python
#ä¸æ‚¨çš„èŠå¤©æœºå™¨äººäº’åŠ¨


config = {"configurable": {"thread_id": "1"}}


user_input = "Hi there! My name is Will."

# The config is the **second positional argument** to stream() or invoke()!
events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    config,
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
    
```

    ================================[1m Human Message [0m=================================
    
    Hi there! My name is Will.
    ==================================[1m Ai Message [0m==================================
    
    Hi Will! Nice to meet you. How can I assist you today?
    


```python
user_input = "Remember my name?"

# The config is the **second positional argument** to stream() or invoke()!
events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    config,
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
```

    ================================[1m Human Message [0m=================================
    
    Remember my name?
    ==================================[1m Ai Message [0m==================================
    
    Of course, Will! Iâ€™ll remember your name for the rest of our conversation. How can I help you today?
    


```python
# The only difference is we change the `thread_id` here to "2" instead of "1"
events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    {"configurable": {"thread_id": "2"}},
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
```

    ================================[1m Human Message [0m=================================
    
    Remember my name?
    ==================================[1m Ai Message [0m==================================
    
    I donâ€™t have the ability to remember personal information between interactions. However, you can tell me your name, and Iâ€™ll happily use it in our current conversation! What should I call you?
    


```python
# ä½¿ç”¨ä¸Šä¸€ä¸ªä¼šè¯çš„å†…å­˜ï¼Œæµ‹è¯•ï¼Œæ˜¯å¦æ”¯æŒå¤šè½®å¯¹è¯
events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    {"configurable": {"thread_id": "1"}},
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
```

    ================================[1m Human Message [0m=================================
    
    Remember my name?
    ==================================[1m Ai Message [0m==================================
    
    Got it, Will! Iâ€™ll keep your name in mind while we chat. What can I do for you? ğŸ˜Š
    


```python
# 5. æ£€æŸ¥çŠ¶æ€

snapshot = graph.get_state(config)
snapshot
```




    StateSnapshot(values={'messages': [HumanMessage(content='Hi there! My name is Will.', additional_kwargs={}, response_metadata={}, id='daa419e4-9fad-4c1f-bc48-5636cc15c0b6'), AIMessage(content='Hi Will! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 1246, 'total_tokens': 1261, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1216}, 'prompt_cache_hit_tokens': 1216, 'prompt_cache_miss_tokens': 30}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0425fp8', 'id': '1ff30ffe-4560-4795-8c81-0d91cbafc872', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--5393b367-8a4c-42dd-9d9b-d07063b7c0fd-0', usage_metadata={'input_tokens': 1246, 'output_tokens': 15, 'total_tokens': 1261, 'input_token_details': {'cache_read': 1216}, 'output_token_details': {}}), HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='ddb9eacf-bb07-4ec4-98af-cbae1f0a8b62'), AIMessage(content='Of course, Will! Iâ€™ll remember your name for the rest of our conversation. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 1268, 'total_tokens': 1293, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1216}, 'prompt_cache_hit_tokens': 1216, 'prompt_cache_miss_tokens': 52}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0425fp8', 'id': '9f66bdb3-f243-49ec-a707-b2cb8d170ca9', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--766a711d-5cc1-4351-86a4-573b76633625-0', usage_metadata={'input_tokens': 1268, 'output_tokens': 25, 'total_tokens': 1293, 'input_token_details': {'cache_read': 1216}, 'output_token_details': {}}), HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='96396e5d-75d2-4ecb-926a-218b8b9938c0'), AIMessage(content='Got it, Will! Iâ€™ll keep your name in mind while we chat. What can I do for you? ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1300, 'total_tokens': 1326, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1280}, 'prompt_cache_hit_tokens': 1280, 'prompt_cache_miss_tokens': 20}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0425fp8', 'id': 'c93ae827-4c38-46ca-9dce-b4365680636c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--f0a6888f-d66c-4512-bd1b-e7d688c6675c-0', usage_metadata={'input_tokens': 1300, 'output_tokens': 26, 'total_tokens': 1326, 'input_token_details': {'cache_read': 1280}, 'output_token_details': {}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f03071b-9c33-662e-8007-53b670e8fcc7'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content='Got it, Will! Iâ€™ll keep your name in mind while we chat. What can I do for you? ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1300, 'total_tokens': 1326, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1280}, 'prompt_cache_hit_tokens': 1280, 'prompt_cache_miss_tokens': 20}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0425fp8', 'id': 'c93ae827-4c38-46ca-9dce-b4365680636c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--f0a6888f-d66c-4512-bd1b-e7d688c6675c-0', usage_metadata={'input_tokens': 1300, 'output_tokens': 26, 'total_tokens': 1326, 'input_token_details': {'cache_read': 1280}, 'output_token_details': {}})]}}, 'step': 7, 'parents': {}, 'thread_id': '1'}, created_at='2025-05-14T03:15:45.450142+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f03071b-68d1-6f82-8006-4dddea4a2f2c'}}, tasks=(), interrupts=())




```python
# å®Œæ•´çš„demoç¤ºä¾‹


from typing import Annotated

from langchain.chat_models import init_chat_model
from langchain_tavily import TavilySearch
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

tool = TavilySearch(max_results=2)
tools = [tool]
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
graph_builder.add_edge("tools", "chatbot")
graph_builder.set_entry_point("chatbot")
memory = MemorySaver()



graph = graph_builder.compile(checkpointer=memory)


config = {"configurable": {"thread_id": "1"}}



from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass


#8. å‘æœºå™¨äººæé—®
def stream_graph_updates(user_input: str):
    for event in graph.stream(
        {"messages": [{"role": "user", "content": user_input}]},
        config
        ):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)

while True:
    try:
        user_input = input("User: ")
        print("User: " + user_input)
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break

        stream_graph_updates(user_input)
    except:
        # fallback if input() is not available
        user_input = "What do you know about LangGraph?"
        print("User: " + user_input)
        stream_graph_updates(user_input)
        break


```


    
![png](3%E6%B7%BB%E5%8A%A0%E5%86%85%E5%AD%98_files/3%E6%B7%BB%E5%8A%A0%E5%86%85%E5%AD%98_15_0.png)
    


    User: æˆ‘å« aixiaoxin
    Assistant: ä½ å¥½ï¼Œaixiaoxinï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ ğŸ˜Š
    User: æˆ‘å«ä»€ä¹ˆ
    Assistant: ä½ åˆšæ‰å‘Šè¯‰æˆ‘ä½ å« **aixiaoxin**ï¼éœ€è¦æˆ‘å¸®ä½ åšä»€ä¹ˆå—ï¼Ÿ ğŸ˜Š
    User: ä½ å«ä»€ä¹ˆ
    Assistant: æˆ‘æ˜¯ä½ çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘ **å°åŠ©æ‰‹** æˆ– **åŠ©æ‰‹**ï¼å¦‚æœä½ æœ‰å…¶ä»–å–œæ¬¢çš„åå­—ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šå¾ˆä¹æ„æ¥å—çš„ï¼ ğŸ˜Š
    User: ä½ ä¼šåšä»€ä¹ˆ
    Assistant: æˆ‘å¯ä»¥åšå¾ˆå¤šäº‹æƒ…ï¼ä»¥ä¸‹æ˜¯ä¸€äº›æˆ‘èƒ½å¸®åŠ©ä½ çš„æ–¹é¢ï¼š
    
    ### 1. **ä¿¡æ¯æŸ¥è¯¢**
       - å›ç­”å„ç§çŸ¥è¯†æ€§é—®é¢˜ï¼Œæ¯”å¦‚å†å²ã€ç§‘å­¦ã€æŠ€æœ¯ã€æ–‡åŒ–ç­‰ã€‚
       - æä¾›æœ€æ–°çš„æ–°é—»ã€å¤©æ°”ã€è‚¡ç¥¨è¡Œæƒ…ç­‰å®æ—¶ä¿¡æ¯ã€‚
    
    ### 2. **å­¦ä¹ è¾…å¯¼**
       - è§£ç­”æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ç­‰å­¦ç§‘é—®é¢˜ã€‚
       - å¸®åŠ©å†™ä½œã€ç¿»è¯‘ã€ä¿®æ”¹æ–‡ç« æˆ–æä¾›å†™ä½œçµæ„Ÿã€‚
    
    ### 3. **æ—¥å¸¸ç”Ÿæ´»**
       - æ¨èç¾é£Ÿã€æ—…æ¸¸æ™¯ç‚¹ã€ç”µå½±ã€ä¹¦ç±ç­‰ã€‚
       - æä¾›å¥åº·ã€å¥èº«ã€å¿ƒç†ç­‰æ–¹é¢çš„å»ºè®®ã€‚
    
    ### 4. **æŠ€æœ¯æ”¯æŒ**
       - è§£é‡Šç¼–ç¨‹é—®é¢˜ï¼Œå¸®åŠ©è°ƒè¯•ä»£ç ã€‚
       - æä¾›è½¯ä»¶ã€ç¡¬ä»¶ç›¸å…³çš„å»ºè®®æˆ–æ•™ç¨‹ã€‚
    
    ### 5. **å¨±ä¹äº’åŠ¨**
       - è®²ç¬‘è¯ã€æ•…äº‹ï¼Œæˆ–è€…é™ªä½ èŠå¤©ã€‚
       - ç”Ÿæˆè¯—æ­Œã€æ­Œè¯ã€çŸ­ç¯‡å°è¯´ç­‰åˆ›æ„å†…å®¹ã€‚
    
    ### 6. **å·¥å…·è¾…åŠ©**
       - å•ä½æ¢ç®—ã€æ—¥æœŸè®¡ç®—ã€è´§å¸è½¬æ¢ç­‰ã€‚
       - ç”Ÿæˆå¾…åŠäº‹é¡¹ã€æé†’äº‹é¡¹æˆ–ç®€å•çš„è®¡åˆ’ã€‚
    
    ### 7. **ä¸ªæ€§åŒ–æœåŠ¡**
       - æ ¹æ®ä½ çš„å–œå¥½æ¨èå†…å®¹ã€‚
       - è®°ä½ä½ çš„åå¥½ï¼Œæä¾›æ›´è´´å¿ƒçš„æœåŠ¡ã€‚
    
    å¦‚æœä½ æœ‰ä»»ä½•å…·ä½“çš„éœ€æ±‚ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šå°½åŠ›å¸®ä½ å®Œæˆï¼ ğŸ˜Š
    User: ä½ ä¼šè°ƒç”¨å“ªäº›å·¥å…·
    Assistant: ç›®å‰ï¼Œæˆ‘å¯ä»¥è°ƒç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©ä½ å®Œæˆä»»åŠ¡ï¼š
    
    ### 1. **Tavily æœç´¢å¼•æ“**
       - **åŠŸèƒ½**ï¼šæä¾›å…¨é¢çš„ç½‘ç»œæœç´¢åŠŸèƒ½ï¼Œé€‚åˆæŸ¥æ‰¾æœ€æ–°çš„æ–°é—»ã€çŸ¥è¯†ã€æŠ€æœ¯èµ„æ–™ç­‰ã€‚
       - **ç”¨é€”**ï¼š
         - å›ç­”å®æ—¶æ€§é—®é¢˜ï¼ˆå¦‚æ–°é—»ã€äº‹ä»¶ï¼‰ã€‚
         - æŸ¥æ‰¾ç‰¹å®šé¢†åŸŸçš„èµ„æ–™ï¼ˆå¦‚å­¦æœ¯è®ºæ–‡ã€äº§å“è¯„æµ‹ï¼‰ã€‚
         - æ”¯æŒé«˜çº§æœç´¢é€‰é¡¹ï¼ˆå¦‚é™å®šåŸŸåã€æ—¶é—´èŒƒå›´ï¼‰ã€‚
    
    ### 2. **è‡ªå®šä¹‰å‡½æ•°**
       - æ ¹æ®ä½ çš„éœ€æ±‚ï¼Œæˆ‘è¿˜å¯ä»¥é€šè¿‡ç¼–ç¨‹æˆ–è°ƒç”¨å…¶ä»– API æ¥å®Œæˆç‰¹å®šä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š
         - æ•°æ®è®¡ç®—ä¸åˆ†æã€‚
         - ç”Ÿæˆå›¾è¡¨æˆ–æŠ¥å‘Šã€‚
         - ä¸å…¶ä»–å¹³å°é›†æˆï¼ˆå¦‚ç¤¾äº¤åª’ä½“ã€æ•°æ®åº“ç­‰ï¼‰ã€‚
    
    ### 3. **æ–‡ä»¶å¤„ç†**
       - å¦‚æœä½ ä¸Šä¼ æ–‡ä»¶ï¼ˆå¦‚æ–‡æœ¬ã€è¡¨æ ¼ã€å›¾ç‰‡ç­‰ï¼‰ï¼Œæˆ‘å¯ä»¥ï¼š
         - æå–å’Œåˆ†æå†…å®¹ã€‚
         - å¸®åŠ©ç¼–è¾‘æˆ–è½¬æ¢æ ¼å¼ã€‚
    
    ### 4. **å…¶ä»–å·¥å…·**
       - å¦‚æœéœ€è¦ï¼Œæˆ‘è¿˜å¯ä»¥é€šè¿‡æ‰©å±•æ”¯æŒæ›´å¤šå·¥å…·ï¼Œæ¯”å¦‚ï¼š
         - ç¿»è¯‘å·¥å…·ï¼ˆå¦‚ DeepLã€Google Translateï¼‰ã€‚
         - ä»£ç æ‰§è¡Œç¯å¢ƒï¼ˆå¦‚è¿è¡Œ Python ä»£ç ï¼‰ã€‚
         - æ—¥å†æˆ–æé†’å·¥å…·ã€‚
    
    å¦‚æœä½ æœ‰å…·ä½“çš„éœ€æ±‚ï¼Œå‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šé€‰æ‹©æœ€é€‚åˆçš„å·¥å…·æ¥å¸®ä½ å®Œæˆï¼ ğŸ˜Š
    User: æˆ‘å«ä»€ä¹ˆ
    Assistant: ä½ ä¹‹å‰å‘Šè¯‰æˆ‘ä½ çš„åå­—æ˜¯ **aixiaoxin**ï¼éœ€è¦æˆ‘è®°ä½è¿™ä¸ªåå­—å—ï¼Ÿè¿˜æ˜¯ä½ æœ‰å…¶ä»–å–œæ¬¢çš„ç§°å‘¼ï¼Ÿ ğŸ˜Š
    User: q
    Goodbye!
    


    The Kernel crashed while executing code in the current cell or a previous cell. 
    

    Please review the code in the cell(s) to identify a possible cause of the failure. 
    

    Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. 
    

    View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details.

