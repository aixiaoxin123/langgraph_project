ÂèÇËÄÉÊñáÊ°£ÊïôÁ®ãÔºö

https://langchain-ai.github.io/langgraph/tutorials/get-started/3-add-memory/



```python
# https://python.langchain.com/docs/integrations/tools/tavily_search/

# Ëé∑ÂèñÂØÜÈí•ÁöÑÂú∞ÂùÄÔºö https://app.tavily.com/sign-in




import os
TAVILY_API_KEY="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"
os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY

```


```python
from langchain_tavily import TavilySearch

tool = TavilySearch(max_results=5)
tools = [tool]
tool.invoke("What's a 'node' in LangGraph?")

```




    {'query': "What's a 'node' in LangGraph?",
     'follow_up_questions': None,
     'answer': None,
     'images': [],
     'results': [{'title': "Mastering LangGraph: A Beginner's Guide to Building ... - Medium",
       'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',
       'content': 'Stateful Graph: LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph. We define nodes for classifying the input, handling greetings, and handling search queries. def classify_input_node(state): LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph.',
       'score': 0.69249696,
       'raw_content': None},
      {'title': 'LangGraph Basics: Understanding State, Schema, Nodes, and Edges',
       'url': 'https://medium.com/@vivekvjnk/langgraph-basics-understanding-state-schema-nodes-and-edges-77f2fd17cae5',
       'content': 'LangGraph Basics: Understanding State, Schema, Nodes, and Edges | by Story_Teller | Medium LangGraph Basics: Understanding State, Schema, Nodes, and Edges LangGraph Basics: Understanding State, Schema, Nodes, and Edges These predefined structures in the messaging app are synonymous with the schema of the state in LangGraph. Just as a messaging app ensures all interactions (messages) follow a consistent format, the schema in LangGraph ensures the state passed along edges is structured and interpretable. This static schema allows nodes to rely on a consistent state format, ensuring seamless communication along edges throughout the graph. In this article, we explored the foundational concepts of graph-based systems, drawing parallels to familiar messaging applications to illustrate how edges, nodes, and state transitions function seamlessly in dynamic workflows.',
       'score': 0.6501347,
       'raw_content': None},
      {'title': "An Absolute Beginner's Guide to LangGraph.js",
       'url': 'https://techcommunity.microsoft.com/blog/educatordeveloperblog/an-absolute-beginners-guide-to-langgraph-js/4212496',
       'content': "We'll add the name and isHuman properties to our State object and update the sayHello and sayBye nodes to use these State object properties. Update the Graph's nodes and edges: We'll build a graph that returns a random fact or joke based on the user's input. Finally, inside src/index.ts, import and execute the Graph with a user input, inside the /joke-or-fact route: In this guide, we've covered the basics of LangGraph.js, building a simple graph that returns a random fact or joke based on user input. We've learned how to define nodes, edges, and state objects and how to add conditional routing to our Graph. LangGraph.js is a powerful tool for building complex workflows and managing State in your applications.",
       'score': 0.59457535,
       'raw_content': None},
      {'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',
       'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',
       'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.',
       'score': 0.5010992,
       'raw_content': None},
      {'title': 'How to Build AI Agents with LangGraph: A Step-by-Step Guide',
       'url': 'https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832',
       'content': 'In this step, we‚Äôll define how the AI agent manages its state (the ongoing context of the conversation) and ensure it responds appropriately to the user‚Äôs input and tool output. This involves creating a template for the conversation, specifying the tools that the assistant will use, and configuring how the AI agent will respond to user input and trigger different functions (like calculating solar savings). This step ensures that the AI assistant can access and trigger the tools as needed during the conversation, creating a seamless interaction between the user and the assistant. By following these steps, you have successfully created an AI assistant using LangGraph that can calculate solar panel energy savings based on user inputs.',
       'score': 0.0924337,
       'raw_content': None}],
     'response_time': 0.5}




```python
import os
from langchain.chat_models import init_chat_model

API_KEY = "sk-123"

BASE_URL = "https://api.deepseek.com"

os.environ["OPENAI_API_KEY"] = API_KEY
os.environ["OPENAI_API_BASE"] = BASE_URL


llm = init_chat_model("openai:deepseek-chat")
```


```python
#Êàë‰ª¨Áé∞Âú®ÂèØ‰ª•Â∞ÜÂÆÉÂêàÂπ∂Âà∞ ÔºöStateGraph

from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

# Modification: tell the LLM which tools it can call
# highlight-next-line
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
```




    <langgraph.graph.state.StateGraph at 0x21246095dc0>




```python
from typing import Annotated

from langchain_tavily import TavilySearch
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

tool = TavilySearch(max_results=2)
tools = [tool]
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
# Any time a tool is called, we return to the chatbot to decide the next step
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()
```


```python

```


```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
```


```python
graph = graph_builder.compile(checkpointer=memory)
```


```python
from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass
```


    
![png](3%E6%B7%BB%E5%8A%A0%E5%86%85%E5%AD%98_files/3%E6%B7%BB%E5%8A%A0%E5%86%85%E5%AD%98_9_0.png)
    



```python
#‰∏éÊÇ®ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫‰∫íÂä®


config = {"configurable": {"thread_id": "1"}}


user_input = "Hi there! My name is Will."

# The config is the **second positional argument** to stream() or invoke()!
events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    config,
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
    
```

    ================================[1m Human Message [0m=================================
    
    Hi there! My name is Will.
    ==================================[1m Ai Message [0m==================================
    
    Hi Will! Nice to meet you. How can I assist you today?
    


```python
user_input = "Remember my name?"

# The config is the **second positional argument** to stream() or invoke()!
events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    config,
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
```

    ================================[1m Human Message [0m=================================
    
    Remember my name?
    ==================================[1m Ai Message [0m==================================
    
    Of course, Will! I‚Äôll remember your name for the rest of our conversation. How can I help you today?
    


```python
# The only difference is we change the `thread_id` here to "2" instead of "1"
events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    {"configurable": {"thread_id": "2"}},
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
```

    ================================[1m Human Message [0m=================================
    
    Remember my name?
    ==================================[1m Ai Message [0m==================================
    
    I don‚Äôt have the ability to remember personal information between interactions. However, you can tell me your name, and I‚Äôll happily use it in our current conversation! What should I call you?
    


```python
# ‰ΩøÁî®‰∏ä‰∏Ä‰∏™‰ºöËØùÁöÑÂÜÖÂ≠òÔºåÊµãËØïÔºåÊòØÂê¶ÊîØÊåÅÂ§öËΩÆÂØπËØù
events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    {"configurable": {"thread_id": "1"}},
    stream_mode="values",
)
for event in events:
    event["messages"][-1].pretty_print()
```

    ================================[1m Human Message [0m=================================
    
    Remember my name?
    ==================================[1m Ai Message [0m==================================
    
    Got it, Will! I‚Äôll keep your name in mind while we chat. What can I do for you? üòä
    


```python
# 5. Ê£ÄÊü•Áä∂ÊÄÅ

snapshot = graph.get_state(config)
snapshot
```




    StateSnapshot(values={'messages': [HumanMessage(content='Hi there! My name is Will.', additional_kwargs={}, response_metadata={}, id='daa419e4-9fad-4c1f-bc48-5636cc15c0b6'), AIMessage(content='Hi Will! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 1246, 'total_tokens': 1261, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1216}, 'prompt_cache_hit_tokens': 1216, 'prompt_cache_miss_tokens': 30}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0425fp8', 'id': '1ff30ffe-4560-4795-8c81-0d91cbafc872', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--5393b367-8a4c-42dd-9d9b-d07063b7c0fd-0', usage_metadata={'input_tokens': 1246, 'output_tokens': 15, 'total_tokens': 1261, 'input_token_details': {'cache_read': 1216}, 'output_token_details': {}}), HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='ddb9eacf-bb07-4ec4-98af-cbae1f0a8b62'), AIMessage(content='Of course, Will! I‚Äôll remember your name for the rest of our conversation. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 1268, 'total_tokens': 1293, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1216}, 'prompt_cache_hit_tokens': 1216, 'prompt_cache_miss_tokens': 52}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0425fp8', 'id': '9f66bdb3-f243-49ec-a707-b2cb8d170ca9', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--766a711d-5cc1-4351-86a4-573b76633625-0', usage_metadata={'input_tokens': 1268, 'output_tokens': 25, 'total_tokens': 1293, 'input_token_details': {'cache_read': 1216}, 'output_token_details': {}}), HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='96396e5d-75d2-4ecb-926a-218b8b9938c0'), AIMessage(content='Got it, Will! I‚Äôll keep your name in mind while we chat. What can I do for you? üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1300, 'total_tokens': 1326, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1280}, 'prompt_cache_hit_tokens': 1280, 'prompt_cache_miss_tokens': 20}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0425fp8', 'id': 'c93ae827-4c38-46ca-9dce-b4365680636c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--f0a6888f-d66c-4512-bd1b-e7d688c6675c-0', usage_metadata={'input_tokens': 1300, 'output_tokens': 26, 'total_tokens': 1326, 'input_token_details': {'cache_read': 1280}, 'output_token_details': {}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f03071b-9c33-662e-8007-53b670e8fcc7'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content='Got it, Will! I‚Äôll keep your name in mind while we chat. What can I do for you? üòä', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1300, 'total_tokens': 1326, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 1280}, 'prompt_cache_hit_tokens': 1280, 'prompt_cache_miss_tokens': 20}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0425fp8', 'id': 'c93ae827-4c38-46ca-9dce-b4365680636c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--f0a6888f-d66c-4512-bd1b-e7d688c6675c-0', usage_metadata={'input_tokens': 1300, 'output_tokens': 26, 'total_tokens': 1326, 'input_token_details': {'cache_read': 1280}, 'output_token_details': {}})]}}, 'step': 7, 'parents': {}, 'thread_id': '1'}, created_at='2025-05-14T03:15:45.450142+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f03071b-68d1-6f82-8006-4dddea4a2f2c'}}, tasks=(), interrupts=())




```python
# ÂÆåÊï¥ÁöÑdemoÁ§∫‰æã


from typing import Annotated

from langchain.chat_models import init_chat_model
from langchain_tavily import TavilySearch
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

tool = TavilySearch(max_results=2)
tools = [tool]
llm_with_tools = llm.bind_tools(tools)

def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=[tool])
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
graph_builder.add_edge("tools", "chatbot")
graph_builder.set_entry_point("chatbot")
memory = MemorySaver()



graph = graph_builder.compile(checkpointer=memory)


config = {"configurable": {"thread_id": "1"}}



from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass


#8. ÂêëÊú∫Âô®‰∫∫ÊèêÈóÆ
def stream_graph_updates(user_input: str):
    for event in graph.stream(
        {"messages": [{"role": "user", "content": user_input}]},
        config
        ):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)

while True:
    try:
        user_input = input("User: ")
        print("User: " + user_input)
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break

        stream_graph_updates(user_input)
    except:
        # fallback if input() is not available
        user_input = "What do you know about LangGraph?"
        print("User: " + user_input)
        stream_graph_updates(user_input)
        break


```


    
![png](3%E6%B7%BB%E5%8A%A0%E5%86%85%E5%AD%98_files/3%E6%B7%BB%E5%8A%A0%E5%86%85%E5%AD%98_15_0.png)
    


    User: ÊàëÂè´ aixiaoxin
    Assistant: ‰Ω†Â•ΩÔºåaixiaoxinÔºÅÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏Æ‰Ω†ÁöÑÂêóÔºü üòä
    User: ÊàëÂè´‰ªÄ‰πà
    Assistant: ‰Ω†ÂàöÊâçÂëäËØâÊàë‰Ω†Âè´ **aixiaoxin**ÔºÅÈúÄË¶ÅÊàëÂ∏Æ‰Ω†ÂÅö‰ªÄ‰πàÂêóÔºü üòä
    User: ‰Ω†Âè´‰ªÄ‰πà
    Assistant: ÊàëÊòØ‰Ω†ÁöÑÊô∫ËÉΩÂä©ÊâãÔºå‰Ω†ÂèØ‰ª•Âè´Êàë **Â∞èÂä©Êâã** Êàñ **Âä©Êâã**ÔºÅÂ¶ÇÊûú‰Ω†ÊúâÂÖ∂‰ªñÂñúÊ¨¢ÁöÑÂêçÂ≠óÔºå‰πüÂèØ‰ª•ÂëäËØâÊàëÔºåÊàë‰ºöÂæà‰πêÊÑèÊé•ÂèóÁöÑÔºÅ üòä
    User: ‰Ω†‰ºöÂÅö‰ªÄ‰πà
    Assistant: ÊàëÂèØ‰ª•ÂÅöÂæàÂ§ö‰∫ãÊÉÖÔºÅ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÊàëËÉΩÂ∏ÆÂä©‰Ω†ÁöÑÊñπÈù¢Ôºö
    
    ### 1. **‰ø°ÊÅØÊü•ËØ¢**
       - ÂõûÁ≠îÂêÑÁßçÁü•ËØÜÊÄßÈóÆÈ¢òÔºåÊØîÂ¶ÇÂéÜÂè≤„ÄÅÁßëÂ≠¶„ÄÅÊäÄÊúØ„ÄÅÊñáÂåñÁ≠â„ÄÇ
       - Êèê‰æõÊúÄÊñ∞ÁöÑÊñ∞Èóª„ÄÅÂ§©Ê∞î„ÄÅËÇ°Á•®Ë°åÊÉÖÁ≠âÂÆûÊó∂‰ø°ÊÅØ„ÄÇ
    
    ### 2. **Â≠¶‰π†ËæÖÂØº**
       - Ëß£Á≠îÊï∞Â≠¶„ÄÅÁâ©ÁêÜ„ÄÅÂåñÂ≠¶Á≠âÂ≠¶ÁßëÈóÆÈ¢ò„ÄÇ
       - Â∏ÆÂä©ÂÜô‰Ωú„ÄÅÁøªËØë„ÄÅ‰øÆÊîπÊñáÁ´†ÊàñÊèê‰æõÂÜô‰ΩúÁÅµÊÑü„ÄÇ
    
    ### 3. **Êó•Â∏∏ÁîüÊ¥ª**
       - Êé®ËçêÁæéÈ£ü„ÄÅÊóÖÊ∏∏ÊôØÁÇπ„ÄÅÁîµÂΩ±„ÄÅ‰π¶Á±çÁ≠â„ÄÇ
       - Êèê‰æõÂÅ•Â∫∑„ÄÅÂÅ•Ë∫´„ÄÅÂøÉÁêÜÁ≠âÊñπÈù¢ÁöÑÂª∫ËÆÆ„ÄÇ
    
    ### 4. **ÊäÄÊúØÊîØÊåÅ**
       - Ëß£ÈáäÁºñÁ®ãÈóÆÈ¢òÔºåÂ∏ÆÂä©Ë∞ÉËØï‰ª£Á†Å„ÄÇ
       - Êèê‰æõËΩØ‰ª∂„ÄÅÁ°¨‰ª∂Áõ∏ÂÖ≥ÁöÑÂª∫ËÆÆÊàñÊïôÁ®ã„ÄÇ
    
    ### 5. **Â®±‰πê‰∫íÂä®**
       - ËÆ≤Á¨ëËØù„ÄÅÊïÖ‰∫ãÔºåÊàñËÄÖÈô™‰Ω†ËÅäÂ§©„ÄÇ
       - ÁîüÊàêËØóÊ≠å„ÄÅÊ≠åËØç„ÄÅÁü≠ÁØáÂ∞èËØ¥Á≠âÂàõÊÑèÂÜÖÂÆπ„ÄÇ
    
    ### 6. **Â∑•ÂÖ∑ËæÖÂä©**
       - Âçï‰ΩçÊç¢ÁÆó„ÄÅÊó•ÊúüËÆ°ÁÆó„ÄÅË¥ßÂ∏ÅËΩ¨Êç¢Á≠â„ÄÇ
       - ÁîüÊàêÂæÖÂäû‰∫ãÈ°π„ÄÅÊèêÈÜí‰∫ãÈ°πÊàñÁÆÄÂçïÁöÑËÆ°Âàí„ÄÇ
    
    ### 7. **‰∏™ÊÄßÂåñÊúçÂä°**
       - Ê†πÊçÆ‰Ω†ÁöÑÂñúÂ•ΩÊé®ËçêÂÜÖÂÆπ„ÄÇ
       - ËÆ∞‰Ωè‰Ω†ÁöÑÂÅèÂ•ΩÔºåÊèê‰æõÊõ¥Ë¥¥ÂøÉÁöÑÊúçÂä°„ÄÇ
    
    Â¶ÇÊûú‰Ω†Êúâ‰ªª‰ΩïÂÖ∑‰ΩìÁöÑÈúÄÊ±ÇÔºåÈöèÊó∂ÂëäËØâÊàëÔºåÊàë‰ºöÂ∞ΩÂäõÂ∏Æ‰Ω†ÂÆåÊàêÔºÅ üòä
    User: ‰Ω†‰ºöË∞ÉÁî®Âì™‰∫õÂ∑•ÂÖ∑
    Assistant: ÁõÆÂâçÔºåÊàëÂèØ‰ª•Ë∞ÉÁî®‰ª•‰∏ãÂ∑•ÂÖ∑Êù•Â∏ÆÂä©‰Ω†ÂÆåÊàê‰ªªÂä°Ôºö
    
    ### 1. **Tavily ÊêúÁ¥¢ÂºïÊìé**
       - **ÂäüËÉΩ**ÔºöÊèê‰æõÂÖ®Èù¢ÁöÑÁΩëÁªúÊêúÁ¥¢ÂäüËÉΩÔºåÈÄÇÂêàÊü•ÊâæÊúÄÊñ∞ÁöÑÊñ∞Èóª„ÄÅÁü•ËØÜ„ÄÅÊäÄÊúØËµÑÊñôÁ≠â„ÄÇ
       - **Áî®ÈÄî**Ôºö
         - ÂõûÁ≠îÂÆûÊó∂ÊÄßÈóÆÈ¢òÔºàÂ¶ÇÊñ∞Èóª„ÄÅ‰∫ã‰ª∂Ôºâ„ÄÇ
         - Êü•ÊâæÁâπÂÆöÈ¢ÜÂüüÁöÑËµÑÊñôÔºàÂ¶ÇÂ≠¶ÊúØËÆ∫Êñá„ÄÅ‰∫ßÂìÅËØÑÊµãÔºâ„ÄÇ
         - ÊîØÊåÅÈ´òÁ∫ßÊêúÁ¥¢ÈÄâÈ°πÔºàÂ¶ÇÈôêÂÆöÂüüÂêç„ÄÅÊó∂Èó¥ËåÉÂõ¥Ôºâ„ÄÇ
    
    ### 2. **Ëá™ÂÆö‰πâÂáΩÊï∞**
       - Ê†πÊçÆ‰Ω†ÁöÑÈúÄÊ±ÇÔºåÊàëËøòÂèØ‰ª•ÈÄöËøáÁºñÁ®ãÊàñË∞ÉÁî®ÂÖ∂‰ªñ API Êù•ÂÆåÊàêÁâπÂÆö‰ªªÂä°ÔºåÊØîÂ¶ÇÔºö
         - Êï∞ÊçÆËÆ°ÁÆó‰∏éÂàÜÊûê„ÄÇ
         - ÁîüÊàêÂõæË°®ÊàñÊä•Âëä„ÄÇ
         - ‰∏éÂÖ∂‰ªñÂπ≥Âè∞ÈõÜÊàêÔºàÂ¶ÇÁ§æ‰∫§Â™í‰Ωì„ÄÅÊï∞ÊçÆÂ∫ìÁ≠âÔºâ„ÄÇ
    
    ### 3. **Êñá‰ª∂Â§ÑÁêÜ**
       - Â¶ÇÊûú‰Ω†‰∏ä‰º†Êñá‰ª∂ÔºàÂ¶ÇÊñáÊú¨„ÄÅË°®Ê†º„ÄÅÂõæÁâáÁ≠âÔºâÔºåÊàëÂèØ‰ª•Ôºö
         - ÊèêÂèñÂíåÂàÜÊûêÂÜÖÂÆπ„ÄÇ
         - Â∏ÆÂä©ÁºñËæëÊàñËΩ¨Êç¢Ê†ºÂºè„ÄÇ
    
    ### 4. **ÂÖ∂‰ªñÂ∑•ÂÖ∑**
       - Â¶ÇÊûúÈúÄË¶ÅÔºåÊàëËøòÂèØ‰ª•ÈÄöËøáÊâ©Â±ïÊîØÊåÅÊõ¥Â§öÂ∑•ÂÖ∑ÔºåÊØîÂ¶ÇÔºö
         - ÁøªËØëÂ∑•ÂÖ∑ÔºàÂ¶Ç DeepL„ÄÅGoogle TranslateÔºâ„ÄÇ
         - ‰ª£Á†ÅÊâßË°åÁéØÂ¢ÉÔºàÂ¶ÇËøêË°å Python ‰ª£Á†ÅÔºâ„ÄÇ
         - Êó•ÂéÜÊàñÊèêÈÜíÂ∑•ÂÖ∑„ÄÇ
    
    Â¶ÇÊûú‰Ω†ÊúâÂÖ∑‰ΩìÁöÑÈúÄÊ±ÇÔºåÂëäËØâÊàëÔºåÊàë‰ºöÈÄâÊã©ÊúÄÈÄÇÂêàÁöÑÂ∑•ÂÖ∑Êù•Â∏Æ‰Ω†ÂÆåÊàêÔºÅ üòä
    User: ÊàëÂè´‰ªÄ‰πà
    Assistant: ‰Ω†‰πãÂâçÂëäËØâÊàë‰Ω†ÁöÑÂêçÂ≠óÊòØ **aixiaoxin**ÔºÅÈúÄË¶ÅÊàëËÆ∞‰ΩèËøô‰∏™ÂêçÂ≠óÂêóÔºüËøòÊòØ‰Ω†ÊúâÂÖ∂‰ªñÂñúÊ¨¢ÁöÑÁß∞ÂëºÔºü üòä
    User: q
    Goodbye!
    


    The Kernel crashed while executing code in the current cell or a previous cell. 
    

    Please review the code in the cell(s) to identify a possible cause of the failure. 
    

    Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. 
    

    View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details.

