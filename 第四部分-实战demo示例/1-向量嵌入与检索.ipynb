{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa8a8b2",
   "metadata": {},
   "source": [
    "参考文档教程：\n",
    "https://www.langchain.com.cn/docs/integrations/text_embedding/ollama/\n",
    "\n",
    "在本教程中，我们将学习如何利用本地ollama的向量模型进行嵌入；以及利用内存数据库进行向量检索。同时附上了如何自定义向量嵌入的教程。\n",
    "\n",
    "#安装依赖：\n",
    "\n",
    "## 安装uv\n",
    "\n",
    "pip install uv -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "\n",
    "## 安装依赖包\n",
    "\n",
    "uv pip install  langgraph \"langchain[openai]\" langchain-community   langchain-ollama -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447550f",
   "metadata": {},
   "source": [
    "\n",
    "# 使用 Ollama 进行嵌入\n",
    "\"\"\"\n",
    "现在我们有了拆分的文档，我们可以将它们索引到用于语义搜索的向量存储中。\n",
    "\"\"\"\n",
    "\n",
    "拉取向量的命令：\n",
    "ollama pull bge-m3:latest\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344cabb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='bge-m3:latest' base_url='192.168.3.136:11434' client_kwargs={} async_client_kwargs={} sync_client_kwargs={} mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None keep_alive=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None\n"
     ]
    }
   ],
   "source": [
    "# 1. 创建向量嵌入模型\n",
    "\n",
    "\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "local_embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3:latest\",\n",
    "    base_url=\"192.168.3.84:11434\",\n",
    ")\n",
    "\n",
    "print(local_embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4672d34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01606138, -0.011700507, 0.0042311954, -0.011310199, -0.017729746, -0.051959366, 0.019131344, 0.0\n"
     ]
    }
   ],
   "source": [
    "# 2 单个嵌入示例\n",
    "\n",
    "text1 = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "single_vector = local_embeddings.embed_query(text1)\n",
    "print(str(single_vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc65a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01606138, -0.011700507, 0.0042311954, -0.011310199, -0.017729746, -0.051959366, 0.019131344, 0.0\n",
      "[-0.07276119, 1.7207447e-05, -0.001866579, -0.03082587, -0.018602427, -0.008962815, -0.019794874, -0\n"
     ]
    }
   ],
   "source": [
    "#3 嵌入多个文本\n",
    "text2 = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "two_vectors = local_embeddings.embed_documents([text, text2])\n",
    "for vector in two_vectors:\n",
    "    print(str(vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea9b6d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.502678] 我是AI小新，你的智能助手，很高兴为你服务。 [{}]\n",
      "* [SIM=0.433895] 我会根据你的需求，提供各种帮助和服务。 [{}]\n"
     ]
    }
   ],
   "source": [
    "# 4 使用内存数据库进行向量检索\n",
    "# https://python.langchain.com/v0.2/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html\n",
    "\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "text1 = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    "text3 = \"我是AI小新，你的智能助手，很高兴为你服务。\"\n",
    "text4 = \"我会根据你的需求，提供各种帮助和服务。\"\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    [text1,text2,text3,text4],\n",
    "    embedding=local_embeddings,\n",
    ")\n",
    "\n",
    "results = vectorstore.similarity_search_with_score(\n",
    "    query=\"你是谁\", k=2,similarity_threshold=0.6\n",
    ")\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81117558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.502678] 我是AI小新，你的智能助手，很高兴为你服务。 [{}]\n",
      "* [SIM=0.433895] 我会根据你的需求，提供各种帮助和服务。 [{}]\n"
     ]
    }
   ],
   "source": [
    "# Use as Retriever:\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":2, \"fetch_k\": 2, \"lambda_mult\": 0.5},\n",
    ")\n",
    "retriever.invoke(\"你是谁?\")\n",
    "\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1833135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5, 0.6, 0.7], [0.5, 0.6, 0.7]]\n",
      "[0.5, 0.6, 0.7]\n"
     ]
    }
   ],
   "source": [
    "# 如果内置的嵌入方法满足不了你，你可以自定义嵌入的方法；\n",
    "\n",
    "\"\"\"\n",
    "自定义嵌入\n",
    "\n",
    "参考文档：https://python.langchain.ac.cn/docs/how_to/custom_embeddings/\n",
    "\n",
    "\n",
    "自定义嵌入\n",
    "LangChain 与许多 第三方嵌入模型 集成。在本指南中，我们将向您展示如何创建自定义 Embedding 类，以防内置的类尚不存在。嵌入在自然语言处理应用中至关重要，因为它们将文本转换为算法可以理解的数字形式，从而实现广泛的应用，例如相似性搜索、文本分类和聚类。\n",
    "\n",
    "使用标准的 Embeddings 接口实现嵌入，将允许您的嵌入在现有的 LangChain 抽象中使用（例如，作为 VectorStore 的驱动嵌入，或使用 CacheBackedEmbeddings 进行缓存）。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#自定义嵌入 的使用 示例\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "\n",
    "class ParrotLinkEmbeddings(Embeddings):\n",
    "    \"\"\"ParrotLink embedding model integration.\n",
    "\n",
    "    # TODO: Populate with relevant params.\n",
    "    Key init args — completion params:\n",
    "        model: str\n",
    "            Name of ParrotLink model to use.\n",
    "\n",
    "    See full list of supported init args and their descriptions in the params section.\n",
    "\n",
    "    # TODO: Replace with relevant init params.\n",
    "    Instantiate:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain_parrot_link import ParrotLinkEmbeddings\n",
    "\n",
    "            embed = ParrotLinkEmbeddings(\n",
    "                model=\"...\",\n",
    "                # api_key=\"...\",\n",
    "                # other params...\n",
    "            )\n",
    "\n",
    "    Embed single text:\n",
    "        .. code-block:: python\n",
    "\n",
    "            input_text = \"The meaning of life is 42\"\n",
    "            embed.embed_query(input_text)\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            # TODO: Example output.\n",
    "\n",
    "    # TODO: Delete if token-level streaming isn't supported.\n",
    "    Embed multiple text:\n",
    "        .. code-block:: python\n",
    "\n",
    "             input_texts = [\"Document 1...\", \"Document 2...\"]\n",
    "            embed.embed_documents(input_texts)\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            # TODO: Example output.\n",
    "\n",
    "    # TODO: Delete if native async isn't supported.\n",
    "    Async:\n",
    "        .. code-block:: python\n",
    "\n",
    "            await embed.aembed_query(input_text)\n",
    "\n",
    "            # multiple:\n",
    "            # await embed.aembed_documents(input_texts)\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            # TODO: Example output.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs.\"\"\"\n",
    "        return [[0.5, 0.6, 0.7] for _ in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "    # optional: add custom async implementations here\n",
    "    # you can also delete these, and the base class will\n",
    "    # use the default implementation, which calls the sync\n",
    "    # version in an async executor:\n",
    "\n",
    "    # async def aembed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "    #     \"\"\"Asynchronous Embed search docs.\"\"\"\n",
    "    #     ...\n",
    "\n",
    "    # async def aembed_query(self, text: str) -> List[float]:\n",
    "    #     \"\"\"Asynchronous Embed query text.\"\"\"\n",
    "    #     ...\n",
    "\n",
    "\n",
    "\n",
    "# 自定义嵌入的示例\n",
    "\n",
    "custom_embeddings = ParrotLinkEmbeddings(\"自定义嵌入模型\")\n",
    "# 多个嵌入的示例\n",
    "print(custom_embeddings.embed_documents([\"我是AI小新\", \"我是自定义嵌入示例\"]))\n",
    "\n",
    "# 单个嵌入的示例\n",
    "print(custom_embeddings.embed_query(\"你是谁\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
